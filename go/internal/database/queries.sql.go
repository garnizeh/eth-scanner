// Code generated by sqlc. DO NOT EDIT.
// versions:
//   sqlc v1.30.0
// source: queries.sql

package database

import (
	"context"
	"database/sql"
	"time"
)

const cleanupStaleJobs = `-- name: CleanupStaleJobs :exec
UPDATE jobs
SET worker_id = NULL, status = 'pending', expires_at = NULL
WHERE status = 'processing'
    AND (
        (last_checkpoint_at IS NOT NULL AND last_checkpoint_at < datetime('now', 'utc', '-' || ?1 || ' seconds'))
        OR (last_checkpoint_at IS NULL AND created_at < datetime('now', 'utc', '-' || ?1 || ' seconds'))
    )
`

// Clear worker assignment for long-stale processing jobs so they can be re-leased.
func (q *Queries) CleanupStaleJobs(ctx context.Context, thresholdSeconds sql.NullString) error {
	_, err := q.db.ExecContext(ctx, cleanupStaleJobs, thresholdSeconds)
	return err
}

const completeBatch = `-- name: CompleteBatch :exec
UPDATE jobs
SET 
    status = 'completed',
    completed_at = datetime('now', 'utc'),
    keys_scanned = ?1,
    duration_ms = ?2,
    current_nonce = nonce_end
WHERE id = ?3 AND worker_id = ?4
`

type CompleteBatchParams struct {
	KeysScanned sql.NullInt64  `json:"keys_scanned"`
	DurationMs  sql.NullInt64  `json:"duration_ms"`
	ID          int64          `json:"id"`
	WorkerID    sql.NullString `json:"worker_id"`
}

// Mark a batch as completed
func (q *Queries) CompleteBatch(ctx context.Context, arg CompleteBatchParams) error {
	_, err := q.db.ExecContext(ctx, completeBatch,
		arg.KeysScanned,
		arg.DurationMs,
		arg.ID,
		arg.WorkerID,
	)
	return err
}

const createBatch = `-- name: CreateBatch :one
INSERT INTO jobs (
    prefix_28, 
    nonce_start, 
    nonce_end,
    current_nonce,
    status, 
    worker_id,
    worker_type,
    expires_at,
    requested_batch_size
)
VALUES (?1, ?2, ?3, ?2, 'processing', ?4, ?5, datetime('now', 'utc', '+' || ?6 || ' seconds'), ?7)
RETURNING id, prefix_28, nonce_start, nonce_end, current_nonce, status, worker_id, worker_type, expires_at, created_at, completed_at, keys_scanned, requested_batch_size, last_checkpoint_at, duration_ms
`

type CreateBatchParams struct {
	Prefix28           []byte         `json:"prefix_28"`
	NonceStart         int64          `json:"nonce_start"`
	NonceEnd           int64          `json:"nonce_end"`
	WorkerID           sql.NullString `json:"worker_id"`
	WorkerType         sql.NullString `json:"worker_type"`
	LeaseSeconds       sql.NullString `json:"lease_seconds"`
	RequestedBatchSize sql.NullInt64  `json:"requested_batch_size"`
}

// Create a new batch (job) for a worker
func (q *Queries) CreateBatch(ctx context.Context, arg CreateBatchParams) (Job, error) {
	row := q.db.QueryRowContext(ctx, createBatch,
		arg.Prefix28,
		arg.NonceStart,
		arg.NonceEnd,
		arg.WorkerID,
		arg.WorkerType,
		arg.LeaseSeconds,
		arg.RequestedBatchSize,
	)
	var i Job
	err := row.Scan(
		&i.ID,
		&i.Prefix28,
		&i.NonceStart,
		&i.NonceEnd,
		&i.CurrentNonce,
		&i.Status,
		&i.WorkerID,
		&i.WorkerType,
		&i.ExpiresAt,
		&i.CreatedAt,
		&i.CompletedAt,
		&i.KeysScanned,
		&i.RequestedBatchSize,
		&i.LastCheckpointAt,
		&i.DurationMs,
	)
	return i, err
}

const createMacroJob = `-- name: CreateMacroJob :one
INSERT INTO jobs (
        prefix_28,
        nonce_start,
        nonce_end,
        current_nonce,
        status,
        worker_id,
        worker_type,
        expires_at,
        requested_batch_size
)
VALUES (?1, ?2, ?3, ?2, 'processing', ?4, ?5, datetime('now', 'utc', '+' || ?6 || ' seconds'), ?7)
RETURNING id, prefix_28, nonce_start, nonce_end, current_nonce, status, worker_id, worker_type, expires_at, created_at, completed_at, keys_scanned, requested_batch_size, last_checkpoint_at, duration_ms
`

type CreateMacroJobParams struct {
	Prefix28           []byte         `json:"prefix_28"`
	NonceStart         int64          `json:"nonce_start"`
	NonceEnd           int64          `json:"nonce_end"`
	WorkerID           sql.NullString `json:"worker_id"`
	WorkerType         sql.NullString `json:"worker_type"`
	LeaseSeconds       sql.NullString `json:"lease_seconds"`
	RequestedBatchSize sql.NullInt64  `json:"requested_batch_size"`
}

// Create a long-lived macro job covering the full nonce space for a prefix
func (q *Queries) CreateMacroJob(ctx context.Context, arg CreateMacroJobParams) (Job, error) {
	row := q.db.QueryRowContext(ctx, createMacroJob,
		arg.Prefix28,
		arg.NonceStart,
		arg.NonceEnd,
		arg.WorkerID,
		arg.WorkerType,
		arg.LeaseSeconds,
		arg.RequestedBatchSize,
	)
	var i Job
	err := row.Scan(
		&i.ID,
		&i.Prefix28,
		&i.NonceStart,
		&i.NonceEnd,
		&i.CurrentNonce,
		&i.Status,
		&i.WorkerID,
		&i.WorkerType,
		&i.ExpiresAt,
		&i.CreatedAt,
		&i.CompletedAt,
		&i.KeysScanned,
		&i.RequestedBatchSize,
		&i.LastCheckpointAt,
		&i.DurationMs,
	)
	return i, err
}

const findAvailableBatch = `-- name: FindAvailableBatch :one
SELECT id, prefix_28, nonce_start, nonce_end, current_nonce, status, worker_id, worker_type, expires_at, created_at, completed_at, keys_scanned, requested_batch_size, last_checkpoint_at, duration_ms FROM jobs
WHERE status = 'pending' 
   OR (status = 'processing' AND (expires_at < datetime('now', 'utc') OR worker_id = ?1))
ORDER BY created_at ASC
LIMIT 1
`

// Find an available batch (pending or expired lease, or already assigned to same worker)
func (q *Queries) FindAvailableBatch(ctx context.Context, workerID sql.NullString) (Job, error) {
	row := q.db.QueryRowContext(ctx, findAvailableBatch, workerID)
	var i Job
	err := row.Scan(
		&i.ID,
		&i.Prefix28,
		&i.NonceStart,
		&i.NonceEnd,
		&i.CurrentNonce,
		&i.Status,
		&i.WorkerID,
		&i.WorkerType,
		&i.ExpiresAt,
		&i.CreatedAt,
		&i.CompletedAt,
		&i.KeysScanned,
		&i.RequestedBatchSize,
		&i.LastCheckpointAt,
		&i.DurationMs,
	)
	return i, err
}

const findIncompleteMacroJob = `-- name: FindIncompleteMacroJob :one
SELECT id, prefix_28, nonce_start, nonce_end, current_nonce, status, worker_id, worker_type, expires_at, created_at, completed_at, keys_scanned, requested_batch_size, last_checkpoint_at, duration_ms FROM jobs
WHERE prefix_28 = ?1
    AND status != 'completed'
ORDER BY created_at ASC
LIMIT 1
`

// Find an existing non-completed (macro) job for a given prefix
func (q *Queries) FindIncompleteMacroJob(ctx context.Context, prefix28 []byte) (Job, error) {
	row := q.db.QueryRowContext(ctx, findIncompleteMacroJob, prefix28)
	var i Job
	err := row.Scan(
		&i.ID,
		&i.Prefix28,
		&i.NonceStart,
		&i.NonceEnd,
		&i.CurrentNonce,
		&i.Status,
		&i.WorkerID,
		&i.WorkerType,
		&i.ExpiresAt,
		&i.CreatedAt,
		&i.CompletedAt,
		&i.KeysScanned,
		&i.RequestedBatchSize,
		&i.LastCheckpointAt,
		&i.DurationMs,
	)
	return i, err
}

const getActiveWorkerDetails = `-- name: GetActiveWorkerDetails :many
SELECT 
    w.id,
    w.worker_type,
    w.last_seen,
    w.total_keys_scanned,
    j.prefix_28 as active_prefix,
    j.current_nonce,
    j.nonce_start,
    j.nonce_end,
    COALESCE(
        (SELECT CAST(j2.keys_scanned AS REAL) / (CAST(j2.duration_ms AS REAL) / 1000.0)
         FROM jobs j2 WHERE j2.id = j.id AND j2.duration_ms > 0),
        (SELECT h.keys_per_second 
         FROM worker_history h 
         WHERE h.worker_id = w.id 
         ORDER BY h.finished_at DESC LIMIT 1)
    ) as last_kps
FROM workers w
LEFT JOIN jobs j ON j.worker_id = w.id AND j.status = 'processing'
WHERE w.last_seen > datetime('now', '-5 minutes')
ORDER BY w.last_seen DESC
`

type GetActiveWorkerDetailsRow struct {
	ID               string        `json:"id"`
	WorkerType       string        `json:"worker_type"`
	LastSeen         time.Time     `json:"last_seen"`
	TotalKeysScanned sql.NullInt64 `json:"total_keys_scanned"`
	ActivePrefix     []byte        `json:"active_prefix"`
	CurrentNonce     sql.NullInt64 `json:"current_nonce"`
	NonceStart       sql.NullInt64 `json:"nonce_start"`
	NonceEnd         sql.NullInt64 `json:"nonce_end"`
	LastKps          interface{}   `json:"last_kps"`
}

// Get detailed info about currently active workers for dashboard
func (q *Queries) GetActiveWorkerDetails(ctx context.Context) ([]GetActiveWorkerDetailsRow, error) {
	rows, err := q.db.QueryContext(ctx, getActiveWorkerDetails)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []GetActiveWorkerDetailsRow{}
	for rows.Next() {
		var i GetActiveWorkerDetailsRow
		if err := rows.Scan(
			&i.ID,
			&i.WorkerType,
			&i.LastSeen,
			&i.TotalKeysScanned,
			&i.ActivePrefix,
			&i.CurrentNonce,
			&i.NonceStart,
			&i.NonceEnd,
			&i.LastKps,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Close(); err != nil {
		return nil, err
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getActiveWorkers = `-- name: GetActiveWorkers :many
SELECT id, worker_type, last_seen, total_keys_scanned, metadata, created_at, updated_at FROM workers
WHERE last_seen > datetime('now', '-' || ? || ' minutes')
ORDER BY last_seen DESC
`

// Get workers active in the last N minutes
func (q *Queries) GetActiveWorkers(ctx context.Context, dollar_1 sql.NullString) ([]Worker, error) {
	rows, err := q.db.QueryContext(ctx, getActiveWorkers, dollar_1)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []Worker{}
	for rows.Next() {
		var i Worker
		if err := rows.Scan(
			&i.ID,
			&i.WorkerType,
			&i.LastSeen,
			&i.TotalKeysScanned,
			&i.Metadata,
			&i.CreatedAt,
			&i.UpdatedAt,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Close(); err != nil {
		return nil, err
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getAllResults = `-- name: GetAllResults :many
SELECT id, private_key, address, worker_id, job_id, nonce_found, found_at FROM results
ORDER BY found_at DESC
LIMIT ?
`

// Get all results (limited)
func (q *Queries) GetAllResults(ctx context.Context, limit int64) ([]Result, error) {
	rows, err := q.db.QueryContext(ctx, getAllResults, limit)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []Result{}
	for rows.Next() {
		var i Result
		if err := rows.Scan(
			&i.ID,
			&i.PrivateKey,
			&i.Address,
			&i.WorkerID,
			&i.JobID,
			&i.NonceFound,
			&i.FoundAt,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Close(); err != nil {
		return nil, err
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getAllWorkerLifetimeStats = `-- name: GetAllWorkerLifetimeStats :many
SELECT 
    worker_id,
    CAST(MAX(worker_type) AS TEXT) as worker_type,
    CAST(SUM(total_batches) AS INTEGER) as total_batches,
    CAST(SUM(total_keys_scanned) AS INTEGER) as total_keys_scanned,
    CAST(SUM(total_duration_ms) AS INTEGER) as total_duration_ms,
    AVG(keys_per_second_avg) as keys_per_second_avg,
    CAST(MAX(keys_per_second_best) AS REAL) as keys_per_second_best,
    CAST(MIN(IFNULL(keys_per_second_worst, 999999999)) AS REAL) as keys_per_second_worst,
    MIN(first_seen_at) as first_seen_at,
    MAX(last_seen_at) as last_seen_at
FROM (
    -- Archived Tier 4
    SELECT 
        worker_id, worker_type, total_batches, total_keys_scanned, total_duration_ms,
        keys_per_second_avg, keys_per_second_best, keys_per_second_worst, first_seen_at, last_seen_at
    FROM worker_stats_lifetime
    
    UNION ALL
    
    -- Recent Tier 1
    SELECT 
        worker_id, worker_type, 1 as total_batches, COALESCE(keys_scanned, 0) as total_keys_scanned, COALESCE(duration_ms, 0) as total_duration_ms,
        keys_per_second as keys_per_second_avg, keys_per_second as keys_per_second_best, keys_per_second as keys_per_second_worst,
        finished_at as first_seen_at, finished_at as last_seen_at
    FROM worker_history
) AS combined
GROUP BY worker_id
ORDER BY total_keys_scanned DESC
`

type GetAllWorkerLifetimeStatsRow struct {
	WorkerID           string          `json:"worker_id"`
	WorkerType         string          `json:"worker_type"`
	TotalBatches       int64           `json:"total_batches"`
	TotalKeysScanned   int64           `json:"total_keys_scanned"`
	TotalDurationMs    int64           `json:"total_duration_ms"`
	KeysPerSecondAvg   sql.NullFloat64 `json:"keys_per_second_avg"`
	KeysPerSecondBest  float64         `json:"keys_per_second_best"`
	KeysPerSecondWorst float64         `json:"keys_per_second_worst"`
	FirstSeenAt        interface{}     `json:"first_seen_at"`
	LastSeenAt         interface{}     `json:"last_seen_at"`
}

// Get unified lifetime stats for all workers, combining archived tier 4 and recent tier 1
func (q *Queries) GetAllWorkerLifetimeStats(ctx context.Context) ([]GetAllWorkerLifetimeStatsRow, error) {
	rows, err := q.db.QueryContext(ctx, getAllWorkerLifetimeStats)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []GetAllWorkerLifetimeStatsRow{}
	for rows.Next() {
		var i GetAllWorkerLifetimeStatsRow
		if err := rows.Scan(
			&i.WorkerID,
			&i.WorkerType,
			&i.TotalBatches,
			&i.TotalKeysScanned,
			&i.TotalDurationMs,
			&i.KeysPerSecondAvg,
			&i.KeysPerSecondBest,
			&i.KeysPerSecondWorst,
			&i.FirstSeenAt,
			&i.LastSeenAt,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Close(); err != nil {
		return nil, err
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getBestDayRecord = `-- name: GetBestDayRecord :one
SELECT stats_date, SUM(total_keys_scanned) as total_keys
FROM (
    SELECT stats_date, total_keys_scanned FROM worker_stats_daily
    UNION ALL
    SELECT date(finished_at) as stats_date, keys_scanned as total_keys_scanned FROM worker_history
)
GROUP BY stats_date
ORDER BY total_keys DESC
LIMIT 1
`

type GetBestDayRecordRow struct {
	StatsDate string          `json:"stats_date"`
	TotalKeys sql.NullFloat64 `json:"total_keys"`
}

// Get the day with highest volume across all workers
func (q *Queries) GetBestDayRecord(ctx context.Context) (GetBestDayRecordRow, error) {
	row := q.db.QueryRowContext(ctx, getBestDayRecord)
	var i GetBestDayRecordRow
	err := row.Scan(&i.StatsDate, &i.TotalKeys)
	return i, err
}

const getBestMonthRecord = `-- name: GetBestMonthRecord :one
SELECT stats_month, SUM(total_keys_scanned) as total_keys
FROM (
    SELECT stats_month, total_keys_scanned FROM worker_stats_monthly
    UNION ALL
    SELECT substr(finished_at, 1, 7) as stats_month, keys_scanned as total_keys_scanned FROM worker_history
)
GROUP BY stats_month
ORDER BY total_keys DESC
LIMIT 1
`

type GetBestMonthRecordRow struct {
	StatsMonth string          `json:"stats_month"`
	TotalKeys  sql.NullFloat64 `json:"total_keys"`
}

// Get the month with highest volume across all workers
func (q *Queries) GetBestMonthRecord(ctx context.Context) (GetBestMonthRecordRow, error) {
	row := q.db.QueryRowContext(ctx, getBestMonthRecord)
	var i GetBestMonthRecordRow
	err := row.Scan(&i.StatsMonth, &i.TotalKeys)
	return i, err
}

const getGlobalDailyStats = `-- name: GetGlobalDailyStats :many
SELECT 
    stats_date,
    SUM(total_batches) as total_batches,
    SUM(total_keys_scanned) as total_keys_scanned,
    SUM(total_duration_ms) as total_duration_ms,
    AVG(keys_per_second_avg) as keys_per_second_avg,
    SUM(error_count) as total_errors
FROM (
    -- Archived historical data
    SELECT 
        stats_date,
        total_batches,
        total_keys_scanned,
        total_duration_ms,
        keys_per_second_avg,
        error_count
    FROM worker_stats_daily
    WHERE stats_date >= substr(?1, 1, 10)

    UNION ALL

    -- Recent history data (not yet pruned/archived)
    SELECT 
        date(finished_at) as stats_date,
        1 as total_batches,
        keys_scanned as total_keys_scanned,
        duration_ms as total_duration_ms,
        keys_per_second as keys_per_second_avg,
        CASE WHEN error_message IS NOT NULL THEN 1 ELSE 0 END as error_count
    FROM worker_history
    WHERE finished_at >= substr(?1, 1, 10)
)
GROUP BY stats_date
ORDER BY stats_date DESC
`

type GetGlobalDailyStatsRow struct {
	StatsDate        string          `json:"stats_date"`
	TotalBatches     sql.NullFloat64 `json:"total_batches"`
	TotalKeysScanned sql.NullFloat64 `json:"total_keys_scanned"`
	TotalDurationMs  sql.NullFloat64 `json:"total_duration_ms"`
	KeysPerSecondAvg sql.NullFloat64 `json:"keys_per_second_avg"`
	TotalErrors      sql.NullFloat64 `json:"total_errors"`
}

// Get daily aggregates for all workers, combining archived and recent history
func (q *Queries) GetGlobalDailyStats(ctx context.Context, sinceDate interface{}) ([]GetGlobalDailyStatsRow, error) {
	rows, err := q.db.QueryContext(ctx, getGlobalDailyStats, sinceDate)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []GetGlobalDailyStatsRow{}
	for rows.Next() {
		var i GetGlobalDailyStatsRow
		if err := rows.Scan(
			&i.StatsDate,
			&i.TotalBatches,
			&i.TotalKeysScanned,
			&i.TotalDurationMs,
			&i.KeysPerSecondAvg,
			&i.TotalErrors,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Close(); err != nil {
		return nil, err
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getGlobalMonthlyStats = `-- name: GetGlobalMonthlyStats :many
SELECT 
    stats_month,
    SUM(total_batches) as total_batches,
    SUM(total_keys_scanned) as total_keys_scanned,
    SUM(total_duration_ms) as total_duration_ms,
    AVG(keys_per_second_avg) as keys_per_second_avg,
    SUM(error_count) as total_errors
FROM (
    -- Archived monthly data
    SELECT 
        stats_month,
        total_batches,
        total_keys_scanned,
        total_duration_ms,
        keys_per_second_avg,
        error_count
    FROM worker_stats_monthly
    WHERE stats_month >= substr(?1, 1, 7)

    UNION ALL

    -- Recent history data (not yet pruned)
    SELECT 
        substr(finished_at, 1, 7) as stats_month,
        1 as total_batches,
        keys_scanned as total_keys_scanned,
        duration_ms as total_duration_ms,
        keys_per_second as keys_per_second_avg,
        CASE WHEN error_message IS NOT NULL AND error_message != '' THEN 1 ELSE 0 END as error_count
    FROM worker_history
    WHERE finished_at >= substr(?1, 1, 7)
)
GROUP BY stats_month
ORDER BY stats_month DESC
`

type GetGlobalMonthlyStatsRow struct {
	StatsMonth       string          `json:"stats_month"`
	TotalBatches     sql.NullFloat64 `json:"total_batches"`
	TotalKeysScanned sql.NullFloat64 `json:"total_keys_scanned"`
	TotalDurationMs  sql.NullFloat64 `json:"total_duration_ms"`
	KeysPerSecondAvg sql.NullFloat64 `json:"keys_per_second_avg"`
	TotalErrors      sql.NullFloat64 `json:"total_errors"`
}

// Get monthly aggregates for all workers, combining archived and recent history
func (q *Queries) GetGlobalMonthlyStats(ctx context.Context, sinceMonth interface{}) ([]GetGlobalMonthlyStatsRow, error) {
	rows, err := q.db.QueryContext(ctx, getGlobalMonthlyStats, sinceMonth)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []GetGlobalMonthlyStatsRow{}
	for rows.Next() {
		var i GetGlobalMonthlyStatsRow
		if err := rows.Scan(
			&i.StatsMonth,
			&i.TotalBatches,
			&i.TotalKeysScanned,
			&i.TotalDurationMs,
			&i.KeysPerSecondAvg,
			&i.TotalErrors,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Close(); err != nil {
		return nil, err
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getJobByID = `-- name: GetJobByID :one
SELECT id, prefix_28, nonce_start, nonce_end, current_nonce, status, worker_id, worker_type, expires_at, created_at, completed_at, keys_scanned, requested_batch_size, last_checkpoint_at, duration_ms FROM jobs
WHERE id = ?
`

// Get a specific job by ID
func (q *Queries) GetJobByID(ctx context.Context, id int64) (Job, error) {
	row := q.db.QueryRowContext(ctx, getJobByID, id)
	var i Job
	err := row.Scan(
		&i.ID,
		&i.Prefix28,
		&i.NonceStart,
		&i.NonceEnd,
		&i.CurrentNonce,
		&i.Status,
		&i.WorkerID,
		&i.WorkerType,
		&i.ExpiresAt,
		&i.CreatedAt,
		&i.CompletedAt,
		&i.KeysScanned,
		&i.RequestedBatchSize,
		&i.LastCheckpointAt,
		&i.DurationMs,
	)
	return i, err
}

const getJobsByPrefix = `-- name: GetJobsByPrefix :many
SELECT 
    id, status, worker_id, worker_type, nonce_start, nonce_end, current_nonce,
    keys_scanned, expires_at, created_at, last_checkpoint_at
FROM jobs
WHERE prefix_28 = ?
ORDER BY created_at DESC
LIMIT 20
`

type GetJobsByPrefixRow struct {
	ID               int64          `json:"id"`
	Status           string         `json:"status"`
	WorkerID         sql.NullString `json:"worker_id"`
	WorkerType       sql.NullString `json:"worker_type"`
	NonceStart       int64          `json:"nonce_start"`
	NonceEnd         int64          `json:"nonce_end"`
	CurrentNonce     sql.NullInt64  `json:"current_nonce"`
	KeysScanned      sql.NullInt64  `json:"keys_scanned"`
	ExpiresAt        sql.NullTime   `json:"expires_at"`
	CreatedAt        time.Time      `json:"created_at"`
	LastCheckpointAt sql.NullTime   `json:"last_checkpoint_at"`
}

// Get all jobs for a specific prefix
func (q *Queries) GetJobsByPrefix(ctx context.Context, prefix28 []byte) ([]GetJobsByPrefixRow, error) {
	rows, err := q.db.QueryContext(ctx, getJobsByPrefix, prefix28)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []GetJobsByPrefixRow{}
	for rows.Next() {
		var i GetJobsByPrefixRow
		if err := rows.Scan(
			&i.ID,
			&i.Status,
			&i.WorkerID,
			&i.WorkerType,
			&i.NonceStart,
			&i.NonceEnd,
			&i.CurrentNonce,
			&i.KeysScanned,
			&i.ExpiresAt,
			&i.CreatedAt,
			&i.LastCheckpointAt,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Close(); err != nil {
		return nil, err
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getJobsByStatus = `-- name: GetJobsByStatus :many
SELECT id, prefix_28, nonce_start, nonce_end, current_nonce, status, worker_id, worker_type, expires_at, created_at, completed_at, keys_scanned, requested_batch_size, last_checkpoint_at, duration_ms FROM jobs
WHERE status = ?
ORDER BY created_at DESC
LIMIT ?
`

type GetJobsByStatusParams struct {
	Status string `json:"status"`
	Limit  int64  `json:"limit"`
}

// Get jobs by status
func (q *Queries) GetJobsByStatus(ctx context.Context, arg GetJobsByStatusParams) ([]Job, error) {
	rows, err := q.db.QueryContext(ctx, getJobsByStatus, arg.Status, arg.Limit)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []Job{}
	for rows.Next() {
		var i Job
		if err := rows.Scan(
			&i.ID,
			&i.Prefix28,
			&i.NonceStart,
			&i.NonceEnd,
			&i.CurrentNonce,
			&i.Status,
			&i.WorkerID,
			&i.WorkerType,
			&i.ExpiresAt,
			&i.CreatedAt,
			&i.CompletedAt,
			&i.KeysScanned,
			&i.RequestedBatchSize,
			&i.LastCheckpointAt,
			&i.DurationMs,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Close(); err != nil {
		return nil, err
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getJobsByWorker = `-- name: GetJobsByWorker :many
SELECT id, prefix_28, nonce_start, nonce_end, current_nonce, status, worker_id, worker_type, expires_at, created_at, completed_at, keys_scanned, requested_batch_size, last_checkpoint_at, duration_ms FROM jobs
WHERE worker_id = ?
ORDER BY created_at DESC
`

// Get all jobs assigned to a specific worker
func (q *Queries) GetJobsByWorker(ctx context.Context, workerID sql.NullString) ([]Job, error) {
	rows, err := q.db.QueryContext(ctx, getJobsByWorker, workerID)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []Job{}
	for rows.Next() {
		var i Job
		if err := rows.Scan(
			&i.ID,
			&i.Prefix28,
			&i.NonceStart,
			&i.NonceEnd,
			&i.CurrentNonce,
			&i.Status,
			&i.WorkerID,
			&i.WorkerType,
			&i.ExpiresAt,
			&i.CreatedAt,
			&i.CompletedAt,
			&i.KeysScanned,
			&i.RequestedBatchSize,
			&i.LastCheckpointAt,
			&i.DurationMs,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Close(); err != nil {
		return nil, err
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getMonthlyStatsByWorker = `-- name: GetMonthlyStatsByWorker :many
SELECT 
    stats_month,
    SUM(total_batches) as total_batches,
    SUM(total_keys_scanned) as total_keys_scanned,
    SUM(total_duration_ms) as total_duration_ms,
    AVG(keys_per_second_avg) as keys_per_second_avg,
    SUM(error_count) as total_errors
FROM (
    -- Archived monthly data
    SELECT 
        stats_month,
        total_batches,
        total_keys_scanned,
        total_duration_ms,
        keys_per_second_avg,
        error_count
    FROM worker_stats_monthly wsm
    WHERE wsm.worker_id = ?1 AND wsm.stats_month >= substr(?2, 1, 7)

    UNION ALL

    -- Recent history data (not yet pruned)
    SELECT 
        substr(finished_at, 1, 7) as stats_month,
        1 as total_batches,
        keys_scanned as total_keys_scanned,
        duration_ms as total_duration_ms,
        keys_per_second as keys_per_second_avg,
        CASE WHEN error_message IS NOT NULL AND error_message != '' THEN 1 ELSE 0 END as error_count
    FROM worker_history wh
    WHERE wh.worker_id = ?1 AND wh.finished_at >= substr(?2, 1, 7)
) AS combined
GROUP BY stats_month
ORDER BY stats_month DESC
`

type GetMonthlyStatsByWorkerParams struct {
	WorkerID   string      `json:"worker_id"`
	SinceMonth interface{} `json:"since_month"`
}

type GetMonthlyStatsByWorkerRow struct {
	StatsMonth       string          `json:"stats_month"`
	TotalBatches     sql.NullFloat64 `json:"total_batches"`
	TotalKeysScanned sql.NullFloat64 `json:"total_keys_scanned"`
	TotalDurationMs  sql.NullFloat64 `json:"total_duration_ms"`
	KeysPerSecondAvg sql.NullFloat64 `json:"keys_per_second_avg"`
	TotalErrors      sql.NullFloat64 `json:"total_errors"`
}

// Get monthly aggregates for a specific worker, combining archived and recent history
func (q *Queries) GetMonthlyStatsByWorker(ctx context.Context, arg GetMonthlyStatsByWorkerParams) ([]GetMonthlyStatsByWorkerRow, error) {
	rows, err := q.db.QueryContext(ctx, getMonthlyStatsByWorker, arg.WorkerID, arg.SinceMonth)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []GetMonthlyStatsByWorkerRow{}
	for rows.Next() {
		var i GetMonthlyStatsByWorkerRow
		if err := rows.Scan(
			&i.StatsMonth,
			&i.TotalBatches,
			&i.TotalKeysScanned,
			&i.TotalDurationMs,
			&i.KeysPerSecondAvg,
			&i.TotalErrors,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Close(); err != nil {
		return nil, err
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getNextNonceRange = `-- name: GetNextNonceRange :one
SELECT MAX(nonce_end) as last_nonce_end
FROM jobs
WHERE prefix_28 = ?1
AND status IN ('processing', 'completed')
`

// Get the next available nonce range for a specific prefix
func (q *Queries) GetNextNonceRange(ctx context.Context, prefix28 []byte) (interface{}, error) {
	row := q.db.QueryRowContext(ctx, getNextNonceRange, prefix28)
	var last_nonce_end interface{}
	err := row.Scan(&last_nonce_end)
	return last_nonce_end, err
}

const getPrefixProgress = `-- name: GetPrefixProgress :many
SELECT 
    prefix_28,
    CAST(SUM(keys_scanned) AS INTEGER) as total_keys_scanned,
    COUNT(DISTINCT worker_id) as worker_count,
    CAST(MIN(created_at) AS TEXT) as started_at,
    CAST(COALESCE(MAX(last_checkpoint_at), MAX(created_at)) AS TEXT) as last_activity_at,
    -- Total keys in a 32-bit nonce range is 2^32 = 4294967296
    CAST((CAST(SUM(keys_scanned) AS REAL) / 4294967296.0 * 100.0) AS REAL) as progress_percentage
FROM jobs
GROUP BY prefix_28
ORDER BY last_activity_at DESC
`

type GetPrefixProgressRow struct {
	Prefix28           []byte  `json:"prefix_28"`
	TotalKeysScanned   int64   `json:"total_keys_scanned"`
	WorkerCount        int64   `json:"worker_count"`
	StartedAt          string  `json:"started_at"`
	LastActivityAt     string  `json:"last_activity_at"`
	ProgressPercentage float64 `json:"progress_percentage"`
}

// Get overall progress for each prefix
func (q *Queries) GetPrefixProgress(ctx context.Context) ([]GetPrefixProgressRow, error) {
	rows, err := q.db.QueryContext(ctx, getPrefixProgress)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []GetPrefixProgressRow{}
	for rows.Next() {
		var i GetPrefixProgressRow
		if err := rows.Scan(
			&i.Prefix28,
			&i.TotalKeysScanned,
			&i.WorkerCount,
			&i.StartedAt,
			&i.LastActivityAt,
			&i.ProgressPercentage,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Close(); err != nil {
		return nil, err
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getPrefixUsage = `-- name: GetPrefixUsage :many
SELECT 
    prefix_28,
    COUNT(*) as total_batches,
    SUM(CASE WHEN status = 'completed' THEN 1 ELSE 0 END) as completed_batches,
    MAX(nonce_end) as highest_nonce,
    SUM(keys_scanned) as total_keys_scanned
FROM jobs
GROUP BY prefix_28
ORDER BY prefix_28
LIMIT ?
`

type GetPrefixUsageRow struct {
	Prefix28         []byte          `json:"prefix_28"`
	TotalBatches     int64           `json:"total_batches"`
	CompletedBatches sql.NullFloat64 `json:"completed_batches"`
	HighestNonce     interface{}     `json:"highest_nonce"`
	TotalKeysScanned sql.NullFloat64 `json:"total_keys_scanned"`
}

// Get usage statistics per prefix
func (q *Queries) GetPrefixUsage(ctx context.Context, limit int64) ([]GetPrefixUsageRow, error) {
	rows, err := q.db.QueryContext(ctx, getPrefixUsage, limit)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []GetPrefixUsageRow{}
	for rows.Next() {
		var i GetPrefixUsageRow
		if err := rows.Scan(
			&i.Prefix28,
			&i.TotalBatches,
			&i.CompletedBatches,
			&i.HighestNonce,
			&i.TotalKeysScanned,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Close(); err != nil {
		return nil, err
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getRecentWorkerHistory = `-- name: GetRecentWorkerHistory :many
SELECT id, worker_id, worker_type, job_id, batch_size, keys_scanned, duration_ms, keys_per_second, prefix_28, nonce_start, nonce_end, finished_at, error_message FROM worker_history
WHERE finished_at > datetime('now', '-' || ? || ' seconds')
ORDER BY finished_at DESC
LIMIT ?
`

type GetRecentWorkerHistoryParams struct {
	Column1 sql.NullString `json:"column_1"`
	Limit   int64          `json:"limit"`
}

// Get recent worker history records for the last N seconds
func (q *Queries) GetRecentWorkerHistory(ctx context.Context, arg GetRecentWorkerHistoryParams) ([]WorkerHistory, error) {
	rows, err := q.db.QueryContext(ctx, getRecentWorkerHistory, arg.Column1, arg.Limit)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []WorkerHistory{}
	for rows.Next() {
		var i WorkerHistory
		if err := rows.Scan(
			&i.ID,
			&i.WorkerID,
			&i.WorkerType,
			&i.JobID,
			&i.BatchSize,
			&i.KeysScanned,
			&i.DurationMs,
			&i.KeysPerSecond,
			&i.Prefix28,
			&i.NonceStart,
			&i.NonceEnd,
			&i.FinishedAt,
			&i.ErrorMessage,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Close(); err != nil {
		return nil, err
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getResultByPrivateKey = `-- name: GetResultByPrivateKey :one
SELECT id, private_key, address, worker_id, job_id, nonce_found, found_at FROM results
WHERE private_key = ?
`

// Find a result by private key
func (q *Queries) GetResultByPrivateKey(ctx context.Context, privateKey string) (Result, error) {
	row := q.db.QueryRowContext(ctx, getResultByPrivateKey, privateKey)
	var i Result
	err := row.Scan(
		&i.ID,
		&i.PrivateKey,
		&i.Address,
		&i.WorkerID,
		&i.JobID,
		&i.NonceFound,
		&i.FoundAt,
	)
	return i, err
}

const getResultsByAddress = `-- name: GetResultsByAddress :many
SELECT id, private_key, address, worker_id, job_id, nonce_found, found_at FROM results
WHERE address = ?
ORDER BY found_at DESC
`

// Find results by Ethereum address
func (q *Queries) GetResultsByAddress(ctx context.Context, address string) ([]Result, error) {
	rows, err := q.db.QueryContext(ctx, getResultsByAddress, address)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []Result{}
	for rows.Next() {
		var i Result
		if err := rows.Scan(
			&i.ID,
			&i.PrivateKey,
			&i.Address,
			&i.WorkerID,
			&i.JobID,
			&i.NonceFound,
			&i.FoundAt,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Close(); err != nil {
		return nil, err
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getResultsByWorker = `-- name: GetResultsByWorker :many
SELECT id, private_key, address, worker_id, job_id, nonce_found, found_at FROM results
WHERE worker_id = ?
ORDER BY found_at DESC
`

// Find results by worker ID
func (q *Queries) GetResultsByWorker(ctx context.Context, workerID string) ([]Result, error) {
	rows, err := q.db.QueryContext(ctx, getResultsByWorker, workerID)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []Result{}
	for rows.Next() {
		var i Result
		if err := rows.Scan(
			&i.ID,
			&i.PrivateKey,
			&i.Address,
			&i.WorkerID,
			&i.JobID,
			&i.NonceFound,
			&i.FoundAt,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Close(); err != nil {
		return nil, err
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getStats = `-- name: GetStats :one
SELECT pending_batches, processing_batches, completed_batches, total_batches, total_keys_scanned, avg_pc_batch_size, avg_esp32_batch_size, results_found, total_workers, active_workers, pc_workers, esp32_workers, global_keys_per_second, active_prefixes FROM stats_summary
`

// Get aggregated statistics
func (q *Queries) GetStats(ctx context.Context) (StatsSummary, error) {
	row := q.db.QueryRowContext(ctx, getStats)
	var i StatsSummary
	err := row.Scan(
		&i.PendingBatches,
		&i.ProcessingBatches,
		&i.CompletedBatches,
		&i.TotalBatches,
		&i.TotalKeysScanned,
		&i.AvgPcBatchSize,
		&i.AvgEsp32BatchSize,
		&i.ResultsFound,
		&i.TotalWorkers,
		&i.ActiveWorkers,
		&i.PcWorkers,
		&i.Esp32Workers,
		&i.GlobalKeysPerSecond,
		&i.ActivePrefixes,
	)
	return i, err
}

const getWorkerByID = `-- name: GetWorkerByID :one
SELECT id, worker_type, last_seen, total_keys_scanned, metadata, created_at, updated_at FROM workers
WHERE id = ?
`

// Get worker information by ID
func (q *Queries) GetWorkerByID(ctx context.Context, id string) (Worker, error) {
	row := q.db.QueryRowContext(ctx, getWorkerByID, id)
	var i Worker
	err := row.Scan(
		&i.ID,
		&i.WorkerType,
		&i.LastSeen,
		&i.TotalKeysScanned,
		&i.Metadata,
		&i.CreatedAt,
		&i.UpdatedAt,
	)
	return i, err
}

const getWorkerDailyStats = `-- name: GetWorkerDailyStats :many
SELECT 
    stats_date,
    SUM(total_batches) as total_batches,
    SUM(total_keys_scanned) as total_keys_scanned,
    SUM(total_duration_ms) as total_duration_ms,
    AVG(keys_per_second_avg) as keys_per_second_avg,
    SUM(error_count) as total_errors
FROM (
    -- Archived historical data
    SELECT 
        stats_date,
        total_batches,
        total_keys_scanned,
        total_duration_ms,
        keys_per_second_avg,
        error_count
    FROM worker_stats_daily wsd
    WHERE wsd.worker_id = ?1 AND wsd.stats_date >= substr(?2, 1, 10)

    UNION ALL

    -- Recent history data (not yet pruned/archived)
    SELECT 
        date(finished_at) as stats_date,
        1 as total_batches,
        keys_scanned as total_keys_scanned,
        duration_ms as total_duration_ms,
        keys_per_second as keys_per_second_avg,
        CASE WHEN error_message IS NOT NULL THEN 1 ELSE 0 END as error_count
    FROM worker_history wh
    WHERE wh.worker_id = ?1 AND wh.finished_at >= substr(?2, 1, 10)
) AS combined
GROUP BY stats_date
ORDER BY stats_date DESC
`

type GetWorkerDailyStatsParams struct {
	WorkerID  string      `json:"worker_id"`
	SinceDate interface{} `json:"since_date"`
}

type GetWorkerDailyStatsRow struct {
	StatsDate        string          `json:"stats_date"`
	TotalBatches     sql.NullFloat64 `json:"total_batches"`
	TotalKeysScanned sql.NullFloat64 `json:"total_keys_scanned"`
	TotalDurationMs  sql.NullFloat64 `json:"total_duration_ms"`
	KeysPerSecondAvg sql.NullFloat64 `json:"keys_per_second_avg"`
	TotalErrors      sql.NullFloat64 `json:"total_errors"`
}

// Get daily aggregates for a worker, combining archived and recent history
// We select and group by stats_date only, as worker_id is filtered to a single value
func (q *Queries) GetWorkerDailyStats(ctx context.Context, arg GetWorkerDailyStatsParams) ([]GetWorkerDailyStatsRow, error) {
	rows, err := q.db.QueryContext(ctx, getWorkerDailyStats, arg.WorkerID, arg.SinceDate)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []GetWorkerDailyStatsRow{}
	for rows.Next() {
		var i GetWorkerDailyStatsRow
		if err := rows.Scan(
			&i.StatsDate,
			&i.TotalBatches,
			&i.TotalKeysScanned,
			&i.TotalDurationMs,
			&i.KeysPerSecondAvg,
			&i.TotalErrors,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Close(); err != nil {
		return nil, err
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getWorkerHistoryLogs = `-- name: GetWorkerHistoryLogs :many
SELECT id, worker_id, worker_type, job_id, batch_size, keys_scanned, duration_ms, keys_per_second, prefix_28, nonce_start, nonce_end, finished_at, error_message FROM worker_history
WHERE worker_id = ?
ORDER BY finished_at DESC
LIMIT ?
`

type GetWorkerHistoryLogsParams struct {
	WorkerID string `json:"worker_id"`
	Limit    int64  `json:"limit"`
}

// Get latest history logs for a specific worker
func (q *Queries) GetWorkerHistoryLogs(ctx context.Context, arg GetWorkerHistoryLogsParams) ([]WorkerHistory, error) {
	rows, err := q.db.QueryContext(ctx, getWorkerHistoryLogs, arg.WorkerID, arg.Limit)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []WorkerHistory{}
	for rows.Next() {
		var i WorkerHistory
		if err := rows.Scan(
			&i.ID,
			&i.WorkerID,
			&i.WorkerType,
			&i.JobID,
			&i.BatchSize,
			&i.KeysScanned,
			&i.DurationMs,
			&i.KeysPerSecond,
			&i.Prefix28,
			&i.NonceStart,
			&i.NonceEnd,
			&i.FinishedAt,
			&i.ErrorMessage,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Close(); err != nil {
		return nil, err
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getWorkerLastPrefix = `-- name: GetWorkerLastPrefix :one
SELECT prefix_28, MAX(nonce_end) as highest_nonce
FROM jobs
WHERE worker_id = ?
GROUP BY prefix_28
ORDER BY MAX(created_at) DESC
LIMIT 1
`

type GetWorkerLastPrefixRow struct {
	Prefix28     []byte      `json:"prefix_28"`
	HighestNonce interface{} `json:"highest_nonce"`
}

// Tracks the last prefix assigned to a worker to enable vertical exhaustion
func (q *Queries) GetWorkerLastPrefix(ctx context.Context, workerID sql.NullString) (GetWorkerLastPrefixRow, error) {
	row := q.db.QueryRowContext(ctx, getWorkerLastPrefix, workerID)
	var i GetWorkerLastPrefixRow
	err := row.Scan(&i.Prefix28, &i.HighestNonce)
	return i, err
}

const getWorkerLifetimeStats = `-- name: GetWorkerLifetimeStats :one
SELECT 
    CAST(SUM(total_batches) AS INTEGER) as total_batches,
    CAST(SUM(total_keys_scanned) AS INTEGER) as total_keys_scanned,
    CAST(SUM(total_duration_ms) AS INTEGER) as total_duration_ms,
    COALESCE(AVG(keys_per_second_avg), 0.0) as keys_per_second_avg,
    COALESCE(MAX(keys_per_second_best), 0.0) as keys_per_second_best
FROM (
    -- Archived data (Tier 4)
    SELECT 
        total_batches, 
        total_keys_scanned, 
        total_duration_ms, 
        keys_per_second_avg, 
        keys_per_second_best
    FROM worker_stats_lifetime
    WHERE worker_stats_lifetime.worker_id = ?1

    UNION ALL

    -- Recent data (Tier 1)
    SELECT 
        1 as total_batches, 
        keys_scanned as total_keys_scanned, 
        duration_ms as total_duration_ms, 
        keys_per_second as keys_per_second_avg, 
        keys_per_second as keys_per_second_best
    FROM worker_history
    WHERE worker_history.worker_id = ?1
) AS unified
`

type GetWorkerLifetimeStatsRow struct {
	TotalBatches      int64       `json:"total_batches"`
	TotalKeysScanned  int64       `json:"total_keys_scanned"`
	TotalDurationMs   int64       `json:"total_duration_ms"`
	KeysPerSecondAvg  interface{} `json:"keys_per_second_avg"`
	KeysPerSecondBest interface{} `json:"keys_per_second_best"`
}

// Get unified lifetime stats for a single worker
func (q *Queries) GetWorkerLifetimeStats(ctx context.Context, workerID string) (GetWorkerLifetimeStatsRow, error) {
	row := q.db.QueryRowContext(ctx, getWorkerLifetimeStats, workerID)
	var i GetWorkerLifetimeStatsRow
	err := row.Scan(
		&i.TotalBatches,
		&i.TotalKeysScanned,
		&i.TotalDurationMs,
		&i.KeysPerSecondAvg,
		&i.KeysPerSecondBest,
	)
	return i, err
}

const getWorkerStats = `-- name: GetWorkerStats :many
SELECT 
    w.id,
    w.worker_type,
    w.total_keys_scanned,
    w.last_seen,
    COUNT(j.id) as total_jobs,
    SUM(CASE WHEN j.status = 'processing' THEN 1 ELSE 0 END) as active_jobs,
    SUM(CASE WHEN j.status = 'completed' THEN 1 ELSE 0 END) as completed_jobs
FROM workers w
LEFT JOIN jobs j ON j.worker_id = w.id
GROUP BY w.id
ORDER BY w.total_keys_scanned DESC
LIMIT ?
`

type GetWorkerStatsRow struct {
	ID               string          `json:"id"`
	WorkerType       string          `json:"worker_type"`
	TotalKeysScanned sql.NullInt64   `json:"total_keys_scanned"`
	LastSeen         time.Time       `json:"last_seen"`
	TotalJobs        int64           `json:"total_jobs"`
	ActiveJobs       sql.NullFloat64 `json:"active_jobs"`
	CompletedJobs    sql.NullFloat64 `json:"completed_jobs"`
}

// Get statistics per worker
func (q *Queries) GetWorkerStats(ctx context.Context, limit int64) ([]GetWorkerStatsRow, error) {
	rows, err := q.db.QueryContext(ctx, getWorkerStats, limit)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []GetWorkerStatsRow{}
	for rows.Next() {
		var i GetWorkerStatsRow
		if err := rows.Scan(
			&i.ID,
			&i.WorkerType,
			&i.TotalKeysScanned,
			&i.LastSeen,
			&i.TotalJobs,
			&i.ActiveJobs,
			&i.CompletedJobs,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Close(); err != nil {
		return nil, err
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getWorkersByType = `-- name: GetWorkersByType :many
SELECT id, worker_type, last_seen, total_keys_scanned, metadata, created_at, updated_at FROM workers
WHERE worker_type = ?
ORDER BY last_seen DESC
`

// Get all workers of a specific type
func (q *Queries) GetWorkersByType(ctx context.Context, workerType string) ([]Worker, error) {
	rows, err := q.db.QueryContext(ctx, getWorkersByType, workerType)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []Worker{}
	for rows.Next() {
		var i Worker
		if err := rows.Scan(
			&i.ID,
			&i.WorkerType,
			&i.LastSeen,
			&i.TotalKeysScanned,
			&i.Metadata,
			&i.CreatedAt,
			&i.UpdatedAt,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Close(); err != nil {
		return nil, err
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const insertResult = `-- name: InsertResult :one
INSERT INTO results (private_key, address, worker_id, job_id, nonce_found)
VALUES (?, ?, ?, ?, ?)
RETURNING id, private_key, address, worker_id, job_id, nonce_found, found_at
`

type InsertResultParams struct {
	PrivateKey string `json:"private_key"`
	Address    string `json:"address"`
	WorkerID   string `json:"worker_id"`
	JobID      int64  `json:"job_id"`
	NonceFound int64  `json:"nonce_found"`
}

// Insert a new result (found key)
func (q *Queries) InsertResult(ctx context.Context, arg InsertResultParams) (Result, error) {
	row := q.db.QueryRowContext(ctx, insertResult,
		arg.PrivateKey,
		arg.Address,
		arg.WorkerID,
		arg.JobID,
		arg.NonceFound,
	)
	var i Result
	err := row.Scan(
		&i.ID,
		&i.PrivateKey,
		&i.Address,
		&i.WorkerID,
		&i.JobID,
		&i.NonceFound,
		&i.FoundAt,
	)
	return i, err
}

const leaseBatch = `-- name: LeaseBatch :execrows
UPDATE jobs
SET 
    status = 'processing',
    worker_id = ?1,
    worker_type = ?2,
    expires_at = datetime('now', 'utc', '+' || ?3 || ' seconds')
WHERE id = ?4 
  AND (status = 'pending' OR (status = 'processing' AND (expires_at < datetime('now', 'utc') OR worker_id IS NULL OR worker_id = ?1)))
`

type LeaseBatchParams struct {
	WorkerID     sql.NullString `json:"worker_id"`
	WorkerType   sql.NullString `json:"worker_type"`
	LeaseSeconds sql.NullString `json:"lease_seconds"`
	ID           int64          `json:"id"`
}

// Lease an existing batch to a worker
func (q *Queries) LeaseBatch(ctx context.Context, arg LeaseBatchParams) (int64, error) {
	result, err := q.db.ExecContext(ctx, leaseBatch,
		arg.WorkerID,
		arg.WorkerType,
		arg.LeaseSeconds,
		arg.ID,
	)
	if err != nil {
		return 0, err
	}
	return result.RowsAffected()
}

const leaseMacroJob = `-- name: LeaseMacroJob :execrows
UPDATE jobs
SET status = 'processing',
        worker_id = ?1,
        worker_type = ?2,
        expires_at = datetime('now', 'utc', '+' || ?3 || ' seconds')
WHERE id = ?4
    AND status != 'completed'
    AND (worker_id IS NULL OR worker_id = ?1 OR expires_at < datetime('now', 'utc'))
`

type LeaseMacroJobParams struct {
	WorkerID     sql.NullString `json:"worker_id"`
	WorkerType   sql.NullString `json:"worker_type"`
	LeaseSeconds sql.NullString `json:"lease_seconds"`
	ID           int64          `json:"id"`
}

// Lease an existing macro job to a worker (if not completed and available)
func (q *Queries) LeaseMacroJob(ctx context.Context, arg LeaseMacroJobParams) (int64, error) {
	result, err := q.db.ExecContext(ctx, leaseMacroJob,
		arg.WorkerID,
		arg.WorkerType,
		arg.LeaseSeconds,
		arg.ID,
	)
	if err != nil {
		return 0, err
	}
	return result.RowsAffected()
}

const recordWorkerStats = `-- name: RecordWorkerStats :exec
INSERT INTO worker_history (
    worker_id, worker_type, job_id, batch_size, keys_scanned, duration_ms, keys_per_second, prefix_28, nonce_start, nonce_end, finished_at, error_message
)
VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
`

type RecordWorkerStatsParams struct {
	WorkerID      string          `json:"worker_id"`
	WorkerType    sql.NullString  `json:"worker_type"`
	JobID         sql.NullInt64   `json:"job_id"`
	BatchSize     sql.NullInt64   `json:"batch_size"`
	KeysScanned   sql.NullInt64   `json:"keys_scanned"`
	DurationMs    sql.NullInt64   `json:"duration_ms"`
	KeysPerSecond sql.NullFloat64 `json:"keys_per_second"`
	Prefix28      []byte          `json:"prefix_28"`
	NonceStart    sql.NullInt64   `json:"nonce_start"`
	NonceEnd      sql.NullInt64   `json:"nonce_end"`
	FinishedAt    time.Time       `json:"finished_at"`
	ErrorMessage  sql.NullString  `json:"error_message"`
}

// Insert a raw worker history record (tier 1)
func (q *Queries) RecordWorkerStats(ctx context.Context, arg RecordWorkerStatsParams) error {
	_, err := q.db.ExecContext(ctx, recordWorkerStats,
		arg.WorkerID,
		arg.WorkerType,
		arg.JobID,
		arg.BatchSize,
		arg.KeysScanned,
		arg.DurationMs,
		arg.KeysPerSecond,
		arg.Prefix28,
		arg.NonceStart,
		arg.NonceEnd,
		arg.FinishedAt,
		arg.ErrorMessage,
	)
	return err
}

const updateCheckpoint = `-- name: UpdateCheckpoint :exec
UPDATE jobs
SET 
    current_nonce = ?1,
    keys_scanned = ?2,
    duration_ms = ?3,
    last_checkpoint_at = datetime('now', 'utc')
WHERE id = ?4 AND worker_id = ?5 AND status = 'processing'
`

type UpdateCheckpointParams struct {
	CurrentNonce sql.NullInt64  `json:"current_nonce"`
	KeysScanned  sql.NullInt64  `json:"keys_scanned"`
	DurationMs   sql.NullInt64  `json:"duration_ms"`
	ID           int64          `json:"id"`
	WorkerID     sql.NullString `json:"worker_id"`
}

// Update job progress checkpoint
func (q *Queries) UpdateCheckpoint(ctx context.Context, arg UpdateCheckpointParams) error {
	_, err := q.db.ExecContext(ctx, updateCheckpoint,
		arg.CurrentNonce,
		arg.KeysScanned,
		arg.DurationMs,
		arg.ID,
		arg.WorkerID,
	)
	return err
}

const updateWorkerKeyCount = `-- name: UpdateWorkerKeyCount :exec
UPDATE workers
SET total_keys_scanned = total_keys_scanned + ?
WHERE id = ?
`

type UpdateWorkerKeyCountParams struct {
	TotalKeysScanned sql.NullInt64 `json:"total_keys_scanned"`
	ID               string        `json:"id"`
}

// Update worker's total key count
func (q *Queries) UpdateWorkerKeyCount(ctx context.Context, arg UpdateWorkerKeyCountParams) error {
	_, err := q.db.ExecContext(ctx, updateWorkerKeyCount, arg.TotalKeysScanned, arg.ID)
	return err
}

const upsertWorker = `-- name: UpsertWorker :exec
INSERT INTO workers (id, worker_type, last_seen, metadata, updated_at)
VALUES (?, ?, datetime('now', 'utc'), ?, datetime('now','utc'))
ON CONFLICT(id) DO UPDATE SET
    last_seen = datetime('now', 'utc'),
    metadata = excluded.metadata,
    updated_at = datetime('now','utc')
`

type UpsertWorkerParams struct {
	ID         string         `json:"id"`
	WorkerType string         `json:"worker_type"`
	Metadata   sql.NullString `json:"metadata"`
}

// Insert or update worker heartbeat
func (q *Queries) UpsertWorker(ctx context.Context, arg UpsertWorkerParams) error {
	_, err := q.db.ExecContext(ctx, upsertWorker, arg.ID, arg.WorkerType, arg.Metadata)
	return err
}
