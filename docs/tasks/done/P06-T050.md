# P06-T050: Implement worker pool with goroutines

**Phase:** P06 - PC Worker - Crypto & Scanning Engine  
**Status:** Completed  
**Priority:** High  
**Dependencies:** P06-T040  
**Estimated Effort:** Large (3h+)

**Completed:** 2026-02-15
**Summary:** Implemented `ScanRangeParallel` which partitions the job's nonce range across all available CPU cores. Verified correctness with unit tests and measured a ~11x speedup on reference hardware.

---

## Description

Implement parallel scanning using a pool of goroutines (one per CPU core using `runtime.NumCPU()`). Each goroutine scans a partition of the nonce range independently, and results are aggregated via channels. This is the primary performance optimization for PC workers.

---

## Acceptance Criteria

- [ ] Function `ScanRangeParallel(ctx context.Context, job Job, targetAddr common.Address) (*ScanResult, error)` implemented
- [ ] Number of workers determined by `runtime.NumCPU()`
- [ ] Nonce range partitioned evenly across workers
- [ ] Each worker runs `ScanRange()` on its partition
- [ ] Result channel aggregates findings from all workers
- [ ] Context cancellation stops all workers immediately
- [ ] Unit tests verify parallel execution and correctness
- [ ] Benchmarks compare single-threaded vs parallel throughput

---

## Implementation Notes

**Key Points:**
- Reference SDD section: 4.3.4 (Parallelization)
- Use `sync.WaitGroup` to wait for all workers to complete
- Use buffered result channel to collect findings
- Handle early termination: if one worker finds match, cancel others
- Partition strategy: divide total nonce range by worker count

**Code Pattern:**
```go
func ScanRangeParallel(ctx context.Context, job Job, targetAddr common.Address) (*ScanResult, error) {
	numWorkers := runtime.NumCPU()
	totalNonces := uint64(job.NonceEnd) - uint64(job.NonceStart) + 1
	noncesPerWorker := totalNonces / uint64(numWorkers)
	
	resultCh := make(chan *ScanResult, numWorkers)
	errCh := make(chan error, numWorkers)
	var wg sync.WaitGroup
	
	ctx, cancel := context.WithCancel(ctx)
	defer cancel()
	
	for i := 0; i < numWorkers; i++ {
		wg.Add(1)
		start := job.NonceStart + uint32(uint64(i)*noncesPerWorker)
		var end uint32
		if i == numWorkers-1 {
			end = job.NonceEnd // last worker takes remainder
		} else {
			end = start + uint32(noncesPerWorker) - 1
		}
		
		go func(workerID int, nonceStart, nonceEnd uint32) {
			defer wg.Done()
			
			workerJob := job
			workerJob.NonceStart = nonceStart
			workerJob.NonceEnd = nonceEnd
			
			result, err := ScanRange(ctx, workerJob, targetAddr)
			if err != nil {
				errCh <- err
				return
			}
			if result != nil {
				resultCh <- result
				cancel() // found match, stop others
			}
		}(i, start, end)
	}
	
	// Wait for all workers
	go func() {
		wg.Wait()
		close(resultCh)
		close(errCh)
	}()
	
	// Collect first result or error
	select {
	case result := <-resultCh:
		return result, nil
	case err := <-errCh:
		return nil, err
	case <-ctx.Done():
		return nil, ctx.Err()
	}
}
```

---

## Testing

- Unit test: verify all workers execute (add logging/counters)
- Unit test: match found in first partition (verify early termination)
- Unit test: match found in last partition
- Unit test: context cancellation stops all workers
- Benchmark: compare throughput vs `ScanRange()` (should scale linearly)

---

## References

- SDD: `docs/architecture/system-design-document.md` (Section 4.3.4)
- Go concurrency patterns: https://go.dev/blog/pipelines
- Related tasks: P06-T060 (partitioning logic), P06-T090 (context handling)
